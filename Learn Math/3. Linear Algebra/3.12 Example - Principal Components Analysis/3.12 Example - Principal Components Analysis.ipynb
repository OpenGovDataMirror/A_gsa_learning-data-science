{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Plot style\n",
    "sns.set()\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".pquote {\n",
       "  text-align: left;\n",
       "  margin: 40px 0 40px auto;\n",
       "  width: 70%;\n",
       "  font-size: 1.5em;\n",
       "  font-style: italic;\n",
       "  display: block;\n",
       "  line-height: 1.3em;\n",
       "  color: #5a75a7;\n",
       "  font-weight: 600;\n",
       "  border-left: 5px solid rgba(90, 117, 167, .1);\n",
       "  padding-left: 6px;\n",
       "}\n",
       ".notes {\n",
       "  font-style: italic;\n",
       "  display: block;\n",
       "  margin: 40px 10%;\n",
       "}\n",
       "img + em {\n",
       "  text-align: center;\n",
       "  display: block;\n",
       "  color: gray;\n",
       "  font-size: 0.9em;\n",
       "  font-weight: 600;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".pquote {\n",
    "  text-align: left;\n",
    "  margin: 40px 0 40px auto;\n",
    "  width: 70%;\n",
    "  font-size: 1.5em;\n",
    "  font-style: italic;\n",
    "  display: block;\n",
    "  line-height: 1.3em;\n",
    "  color: #5a75a7;\n",
    "  font-weight: 600;\n",
    "  border-left: 5px solid rgba(90, 117, 167, .1);\n",
    "  padding-left: 6px;\n",
    "}\n",
    ".notes {\n",
    "  font-style: italic;\n",
    "  display: block;\n",
    "  margin: 40px 10%;\n",
    "}\n",
    "img + em {\n",
    "  text-align: center;\n",
    "  display: block;\n",
    "  color: gray;\n",
    "  font-size: 0.9em;\n",
    "  font-weight: 600;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotVectors(vecs, cols, alpha=1):\n",
    "    \"\"\"\n",
    "    Plot set of vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vecs : array-like\n",
    "        Coordinates of the vectors to plot. Each vectors is in an array. For\n",
    "        instance: [[1, 3], [2, 2]] can be used to plot 2 vectors.\n",
    "    cols : array-like\n",
    "        Colors of the vectors. For instance: ['red', 'blue'] will display the\n",
    "        first vector in red and the second in blue.\n",
    "    alpha : float\n",
    "        Opacity of vectors\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    fig : instance of matplotlib.figure.Figure\n",
    "        The figure of the vectors\n",
    "    \"\"\"\n",
    "    plt.axvline(x=0, color='#A9A9A9', zorder=0)\n",
    "    plt.axhline(y=0, color='#A9A9A9', zorder=0)\n",
    "\n",
    "    for i in range(len(vecs)):\n",
    "        if (isinstance(alpha, list)):\n",
    "            alpha_i = alpha[i]\n",
    "        else:\n",
    "            alpha_i = alpha\n",
    "        x = np.concatenate([[0,0],vecs[i]])\n",
    "        plt.quiver([x[0]],\n",
    "                   [x[1]],\n",
    "                   [x[2]],\n",
    "                   [x[3]],\n",
    "                   angles='xy', scale_units='xy', scale=1, color=cols[i],\n",
    "                  alpha=alpha_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand\\norm[1]{\\left\\lVert#1\\right\\rVert} \n",
    "\\DeclareMathOperator{\\Tr}{Tr}\n",
    "\\newcommand\\bs[1]{\\boldsymbol{#1}}\n",
    "\\newcommand\\argmin[1]{\\underset{\\bs{#1}}{\\arg\\min}}\n",
    "\\newcommand\\argmax[1]{\\underset{\\bs{#1}}{\\arg\\max}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this final chapter, we'll put together everything we've learning about linear algebra through an application of  Principal Components Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.12 Principal Components Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Dimensions** are a crucial topic in data science. The dimensions of a dataset are all of the features (aka variables) of the dataset. For instance, take a look at the [iris dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), a common classification challenge for machine learning models:\n",
    "\n",
    "<img src=\"images/iris.jpg\">\n",
    "<em> The famed iris dataset </em>\n",
    "\n",
    "In this dataset, the rows are the samples (i.e. individual flowers) and the columns are:\n",
    " - Sepal Length\n",
    " - Sepal Width\n",
    " - Petal Length\n",
    " - Petal Width\n",
    " - Species\n",
    "\n",
    "Since the object is to predict the species, Sepal Length, Sepal Width, Petal Length and Petal Width are all dimensions. With so few features, it is easy to plot a couple at a time to visualize a potential relationship:\n",
    "\n",
    "<img src=\"images/sphx_glr_plot_iris_dataset_002.png\" width = 450 height = 450 >\n",
    "<em> Sepal Length plotted against Sepal Width </em>\n",
    "\n",
    "But sometimes your dataset has a lot of dimensions - hundreds and maybe even thousands. This can be a problem for reasons that have been summed up as the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). The gist of the curse is that not every sample will possess a value for every dimension when you have hundreds or thousands of dimensions. Such sparsity is problematic for any method that requires statistical significance; it can also obfuscate meaningful patterns that would otherwise be easy to detect. \n",
    "\n",
    "The natural solution to the curse of dimensionality is to reduce the number of dimensions in your dataset. But which dimensions should you drop? With hundreds or thousdands of dimensions, it's likely that some of the dimensions will be correlated with one another. When that's the case, they're not really adding anything new information-wise. So the right approach might be dropping dimensions that are highly correlated with others. But the problem here is that you might be losing potentially good information with every dimension you drop, especially when you consider the fact that out-of-sample observtions might be well-explained by these dimensions. Thus your dilemma is to find a way to reduce your dimensionality without losing all of the information present in the dataset as-is.\n",
    "\n",
    "Enter principal components analysis (PCA). The aim of PCA is to reduce the number of dimensions of a dataset by providing us with a new set of dimensions - called the principal components (PC). These PCs are ordered, with the first PC explaining the largest amount of variance among the original set of dimensions. In addition, each PC is *orthogonal* to the preceding one. Remember that orthogonal vectors possess the property that their dot product is equal to $0$. A handy way to remember/visualize this is to note that the dictionary definition of orthogonal means \"of or involving right angles; at right angles.\" Visually, you can see that these unit vectors are an example of orthogonal vectors:\n",
    "\n",
    "<img src=\"images/orthogonal-vectors.png\" width=\"200\" alt=\"Example of orthogonal vectors\" title=\"Orthogonal vectors\">\n",
    "<em>Orthogonal vectors</em>\n",
    "\n",
    "\n",
    "All this this means is that each PC is [decorrelated](https://en.wikipedia.org/wiki/Decorrelation) to the preceding one. The upshot is that you can choose the number of principal components that you want to reduce your highly dimensional dataset down to (although there are methods that can help you with this choice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Describing the problem\n",
    "\n",
    "The problem can be expressed as finding a function that converts a set of data points from $\\mathbb{R}^n$ to $\\mathbb{R}^l$. This means that we change the number of dimensions of our dataset. We also need a function that can decode back from the transformed dataset to the initial one:\n",
    "\n",
    "<img src=\"images/principal-components-analysis-PCA-change-coordinates.png\" width=\"80%\" alt=\"Principal components analysis (PCA)\" title=\"Principal components analysis (PCA)\">\n",
    "<em>Principal components analysis as a change of coordinate system</em>\n",
    "\n",
    "The first step is to understand the shape of the data. $x^{(i)}$ is one data point containing $n$ dimensions. Let's have $m$ data points organized as column vectors (one column per point):\n",
    "\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    x^{(1)} & x^{(2)} & \\cdots & x^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we deploy the $n$ dimensions of our data points we will have:\n",
    "\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    x_1^{(1)} & x_1^{(2)} & \\cdots & x_1^{(m)}\\\\\\\\\n",
    "    x_2^{(1)} & x_2^{(2)} & \\cdots & x_2^{(m)}\\\\\\\\\n",
    "    \\cdots & \\cdots & \\cdots & \\cdots\\\\\\\\\n",
    "    x_n^{(1)} & x_n^{(2)} & \\cdots & x_n^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can also write:\n",
    "\n",
    "$$\n",
    "\\bs{x}=\n",
    "\\begin{bmatrix}\n",
    "    x_1\\\\\\\\\n",
    "    x_2\\\\\\\\\n",
    "    \\cdots\\\\\\\\\n",
    "    x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$c$ will have the shape:\n",
    "\n",
    "$$\n",
    "\\bs{c}=\n",
    "\\begin{bmatrix}\n",
    "    c_1\\\\\\\\\n",
    "    c_2\\\\\\\\\n",
    "    \\cdots\\\\\\\\\n",
    "    c_l\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Adding some constraints: the decoding function\n",
    "\n",
    "The encoding function $f(\\bs{x})$ transforms $\\bs{x}$ into $\\bs{c}$ and the decoding function transforms back $\\bs{c}$ into an approximation of $\\bs{x}$. To keep things simple, PCA will respect some constraints:\n",
    "\n",
    "### Constraint 1.\n",
    "\n",
    "The decoding function has to be a simple matrix multiplication:\n",
    "\n",
    "$$\n",
    "g(\\bs{c})=\\bs{Dc}\n",
    "$$\n",
    "\n",
    "By applying the matrix $\\bs{D}$ to the dataset from the new coordinates system we should get back to the initial coordinate system.\n",
    "\n",
    "### Constraint 2.\n",
    "\n",
    "The columns of $\\bs{D}$ must be orthogonal.\n",
    "\n",
    "### Constraint 3.\n",
    "\n",
    "The columns of $\\bs{D}$ must have unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the encoding function\n",
    "\n",
    "Important: For now we will consider only **one data point**. Thus we will have the following dimensions for these matrices (note that $\\bs{x}$ and $\\bs{c}$ are column vectors):\n",
    "\n",
    "<img src=\"images/principal-components-analysis-PCA-decoding-function.png\" width=\"250\" alt=\"Principal components analysis (PCA) - the decoding function\" title=\"The decoding function\">\n",
    "<em>The decoding function</em>\n",
    "\n",
    "We want a decoding function which is a simple matrix multiplication. For that reason, we have $g(\\bs{c})=\\bs{Dc}$. We will then find the encoding function from the decoding function. We want to minimize the error between the decoded data point and the actual data point. With our previous notation, this means reducing the distance between $\\bs{x}$ and $g(\\bs{c})$. As an indicator of this distance, we will use the squared $L^2$ norm:\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{x} - g(\\bs{c})}_2^2\n",
    "$$\n",
    "\n",
    "This is what we want to minimize. Let's call $\\bs{c}^*$ the optimal $\\bs{c}$. Mathematically it can be written:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} \\norm{\\bs{x} - g(\\bs{c})}_2^2\n",
    "$$\n",
    "\n",
    "This means that we want to find the values of the vector $\\bs{c}$ such that $\\norm{\\bs{x} - g(\\bs{c})}_2^2$ is as small as possible.\n",
    "\n",
    "If you have a look back to our lesson on vector norms, you'll see that the squared $L^2$ norm can be expressed as:\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{y}}_2^2 = \\bs{y}^\\text{T}\\bs{y}\n",
    "$$\n",
    "\n",
    "We have named the variable $\\bs{y}$ to avoid confusion with our $\\bs{x}$. Here $\\bs{y}=\\bs{x} - g(\\bs{c})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the equation that we want to minimize becomes:\n",
    "\n",
    "$$\n",
    "(\\bs{x} - g(\\bs{c}))^\\text{T}(\\bs{x} - g(\\bs{c}))\n",
    "$$\n",
    "\n",
    "Since the transpose respects addition we have:\n",
    "\n",
    "$$\n",
    "(\\bs{x}^\\text{T} - g(\\bs{c})^\\text{T})(\\bs{x} - g(\\bs{c}))\n",
    "$$\n",
    "\n",
    "By the distributive property, we can develop:\n",
    "\n",
    "$$\n",
    "\\bs{x^\\text{T}x} - \\bs{x}^\\text{T}g(\\bs{c}) -  g(\\bs{c})^\\text{T}\\bs{x} + g(\\bs{c})^\\text{T}g(\\bs{c})\n",
    "$$\n",
    "\n",
    "The commutative property tells us that $\n",
    "\\bs{x^\\text{T}y} = \\bs{y^\\text{T}x}\n",
    "$. We can use that in the previous equation: $\n",
    "\\bs{x}^\\text{T}g(\\bs{c}) = g(\\bs{c})^\\text{T}\\bs{x}\n",
    "$. So the equation becomes:\n",
    "\n",
    "$$\n",
    "\\bs{x^\\text{T}x} -2\\bs{x}^\\text{T}g(\\bs{c}) + g(\\bs{c})^\\text{T}g(\\bs{c})\n",
    "$$\n",
    "\n",
    "The first term $\\bs{x^\\text{T}x}$ does not depends on $\\bs{c}$ and since we want to minimize the function according to $\\bs{c}$ we can just nix this term. We simplify to:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}g(\\bs{c}) + g(\\bs{c})^\\text{T}g(\\bs{c})\n",
    "$$\n",
    "\n",
    "Since $g(\\bs{c})=\\bs{Dc}$:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}\\bs{Dc} + (\\bs{Dc})^\\text{T}\\bs{Dc}\n",
    "$$\n",
    "\n",
    "Since $(\\bs{Dc})^\\text{T}=\\bs{c}^\\text{T}\\bs{D}^\\text{T}$, we have:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{D}^\\text{T}\\bs{Dc}\n",
    "$$\n",
    "\n",
    "As we saw in [2.6](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.6-Special-Kinds-of-Matrices-and-Vectors/), $\\bs{D}^\\text{T}\\bs{D}=\\bs{I}_l$ because $\\bs{D}$ is orthogonal (actually, it is [semi-orthogonal](https://en.wikipedia.org/wiki/Semi-orthogonal_matrix) if $n \\neq l$) and their columns have unit norm. We can replace in the equation:\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{I}_l\\bs{c}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bs{c}^* = \\underset{c}{\\arg\\min} -2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{c}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the function\n",
    "\n",
    "Now the goal is to find the minimum of the function $- 2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{c}$. One widely used way of doing that is to use the **gradient descent** algorithm. It is not the focus of this chapter but we will say a word about it.\n",
    "\n",
    "The main idea is that the sign of the [derivative](https://en.wikipedia.org/wiki/Derivative) of the function at a specific value of $x$ tells you if you need to increase or decrease $x$ to reach the minimum. When the slope is near $0$, the minimum should have been reached.\n",
    "\n",
    "<img src=\"images/gradient-descent.png\" width=\"400\" alt=\"Mechanism of the gradient descent algorithm\" title=\"Mechanism of the gradient descent algorithm\">\n",
    "<em>Gradient descent</em>\n",
    "\n",
    "However, functions with local minima can trouble the descent:\n",
    "\n",
    "<img src=\"images/gradient-descent-local-minima.png\" width=\"400\" alt=\"Gradient descent in the case of local minimum\" title=\"Gradient descent\">\n",
    "<em>Gradient descent can get stuck in local minima</em>\n",
    "\n",
    "These examples are in 2 dimensions but the principle stands for higher dimensional functions as well. In higher dimensions, the gradient is a vector containing the [partial derivatives](https://en.wikipedia.org/wiki/Partial_derivative) of all dimensions. Its mathematical notation is $\\nabla_xf(\\bs{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the gradient of the function\n",
    "\n",
    "Here we want to minimize through each dimension of $\\bs{c}$. We are looking for a slope of $0$. The equation is:\n",
    "\n",
    "$$\n",
    "\\nabla_c(-2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{c})=0\n",
    "$$\n",
    "\n",
    "Let's take these terms separately to calculate the derivative according to $\\bs{c}$. \n",
    "\n",
    "$$\n",
    "\\frac{d(-2\\bs{x}^\\text{T}\\bs{Dc})}{d\\bs{c}} = -2\\bs{x}^\\text{T}\\bs{D}\n",
    "$$\n",
    "\n",
    "The second term is $\\bs{c}^\\text{T}\\bs{c}$. We can develop the vector $\\bs{c}$ and calculate the derivative for each element:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{d(\\bs{c}\\text{T}\\bs{c})}{d\\bs{c}} &=\n",
    "\\left(\\frac{d(\\bs{c}_1^2 + \\bs{c}_2^2 + \\cdots + \\bs{c}_l^2)}{d\\bs{c}_1},\n",
    "\\frac{d(\\bs{c}_1^2 + \\bs{c}_2^2 + \\cdots + \\bs{c}_l^2)}{d\\bs{c}_2},\n",
    "\\cdots,\n",
    "\\frac{d(\\bs{c}_1^2 + \\bs{c}_2^2 + \\cdots + \\bs{c}_l^2)}{d\\bs{c}_l}\\right) \\\\\\\\\n",
    "&=(2\\bs{c}_1, 2\\bs{c}_2, \\cdots, 2\\bs{c}_l)\\\\\\\\\n",
    "&=2(\\bs{c}_1, \\bs{c}_2, \\cdots, \\bs{c}_l)\\\\\\\\\n",
    "&=2\\bs{c}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So we can plug in our derivatives:\n",
    "\n",
    "$$\n",
    "\\nabla_c(-2\\bs{x}^\\text{T}\\bs{Dc} + \\bs{c}^\\text{T}\\bs{c})=0\\\\\\\\\n",
    "-2\\bs{x}^\\text{T}\\bs{D} + 2\\bs{c}=0\\\\\\\\\n",
    "-2\\bs{D}^\\text{T}\\bs{x} + 2\\bs{c}=0\\\\\\\\\n",
    "\\bs{c}=\\bs{D}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "Great! We found the encoding function! Here are its dimensions:\n",
    "\n",
    "<img src=\"images/principal-components-analysis-PCA-encoding-function.png\" width=\"250\" alt=\"Expression of the encoding function\" title=\"The encoding function\">\n",
    "<em>The encoding function</em>\n",
    "\n",
    "To go back from $\\bs{c}$ to $\\bs{x}$ we use $g(\\bs{c})=\\bs{Dc}$:\n",
    "\n",
    "$$\n",
    "r(\\bs{x}) = g(f(\\bs{x})=\\bs{D}\\bs{D}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "<img src=\"images/principal-components-analysis-PCA-reconstruction-function.png\" width=\"300\" alt=\"Expression of the reconstruction function\" title=\"The reconstruction function\">\n",
    "<em>The reconstruction function</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding $\\bs{D}$\n",
    "\n",
    "The next step is to find the matrix $\\bs{D}$. Recall that the purpose of the PCA is to change the coordinate system in order to maximize the variance along the first dimensions of the projected space. This is equivalent to minimizing the error between data points and their reconstruction ([cf here](https://stats.stackexchange.com/questions/32174/pca-objective-function-what-is-the-connection-between-maximizing-variance-and-m))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Frobenius norm\n",
    "\n",
    "Since we have to take all points into account (the same matrix $\\bs{D}$ will be used for all points) we will use the Frobenius norm of the errors, which is equivalent to the $L^2$ norm for matrices. Here's the formula for the Frobenius norm:\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{A}}_F=\\sqrt{\\sum_{i,j}A^2_{i,j}}\n",
    "$$\n",
    "\n",
    "It is like if you unroll the matrix to end up with a one dimensional vector and that you take the $L^2$ norm of this vector.\n",
    "\n",
    "We will call $\\bs{D}^*$ the optimal $\\bs{D}$ (in the sense that the error is as small as possible). We have:\n",
    "\n",
    "$$\n",
    "\\bs{D}^* = \\underset{\\bs{D}}{\\arg\\min} \\sqrt{\\sum_{i,j}(x_j^{(i)}-r(\\bs{x}^{(i)})_j})^2\n",
    "$$\n",
    "\n",
    "With the constraint that $\\bs{D}^\\text{T}\\bs{D}=\\bs{I}_l$ because we have chosen the constraint of having the columns of $\\bs{D}$ orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first principal component\n",
    "\n",
    "We will start to find only the first principal component (PC). For that reason, we will have $l=1$. So the matrix $\\bs{D}$ will have the shape $(n \\times 1)$: it is a simple column vector. Since it is a vector we will call it $\\bs{d}$:\n",
    "\n",
    "<img src=\"images/first-principal-component.png\" width=\"100\" alt=\"Dimension of the first principal component\" title=\"The first principal component\">\n",
    "<em>The first principal component</em>\n",
    "\n",
    "We can therefore remove the sum over $j$ and the square root since we will take the squared $L^2$ norm:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{(\\bs{x}^{(i)}-r(\\bs{x}^{(i)}))}_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "We have also seen that:\n",
    "\n",
    "$$\n",
    "r(\\bs{x})=\\bs{D}\\bs{D}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "Since we are looking only for the first PC:\n",
    "\n",
    "$$\n",
    "r(\\bs{x})=\\bs{d}\\bs{d}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "We can plug $r(\\bs{x})$ into the equation:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{\\bs{x}^{(i)}-\\bs{dd}^\\text{T}\\bs{x}^{(i)}}_2^2\n",
    "$$\n",
    "\n",
    "Because of constraint 3 (the columns of $\\bs{D}$ must have unit norms), we have $\\norm{\\bs{d}}_2 = 1$. $\\bs{d}$ is one of the columns of $\\bs{D}$ and thus has a unit norm.\n",
    "\n",
    "\n",
    "Instead of using the sum along the $m$ data points $\\bs{x}$ we can have the matrix $\\bs{X}$ gather all of the observations:\n",
    "\n",
    "$$\n",
    "\\bs{X} = \\begin{bmatrix}\n",
    "    \\bs{x}^{(1)\\text{T}}\\\\\\\\\n",
    "    \\bs{x}^{(2)\\text{T}}\\\\\\\\\n",
    "    \\cdots\\\\\\\\\n",
    "    \\bs{x}^{(m)\\text{T}}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "    \\bs{x}_1^{(1)} & \\bs{x}_2^{(1)} & \\cdots & \\bs{x}_n^{(1)}\\\\\\\\\n",
    "    \\bs{x}_1^{(2)} & \\bs{x}_2^{(2)} & \\cdots & \\bs{x}_n^{(2)}\\\\\\\\\n",
    "    \\cdots & \\cdots & \\cdots & \\cdots\\\\\\\\\n",
    "    \\bs{x}_0^{(m)} & \\bs{x}_1^{(m)} & \\cdots & \\bs{x}_n^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want $\\bs{x}^{(i)\\text{T}}$ instead of $\\bs{x}^{(i)}$ in our expression of $\\bs{d}^*$. We can transpose the content of the norm:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bs{d}^* &= \\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{(\\bs{x}^{(i)}-\\bs{dd}^\\text{T}\\bs{x}^{(i)})^\\text{T}}_2^2\\\\\\\\\n",
    "&=\\underset{\\bs{d}}{\\arg\\min} \\sum_{i}\\norm{\\bs{x}^{(i)\\text{T}}-\\bs{x}^{(i)\\text{T}}\\bs{dd}^\\text{T}}_2^2\\\\\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\norm{\\bs{X}-\\bs{X}\\bs{dd}^\\text{T}}_\\text{F}^2\n",
    "$$\n",
    "\n",
    "with the constraint that $\\bs{dd}^\\text{T}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Trace operator\n",
    "\n",
    "We will now use the Trace operator to simplify the equation. Recall that:\n",
    "\n",
    "$$\n",
    "\\norm{\\bs{A}}_F=\\sqrt{\\Tr({\\bs{AA}^T})}\n",
    "$$\n",
    "\n",
    "So here $\\bs{A}=\\bs{X}-\\bs{X}\\bs{dd}^\\text{T}$. So we have:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\underset{\\bs{d}}{\\arg\\min} \\Tr{((\\bs{X}-\\bs{Xdd}^\\text{T})}(\\bs{X}-\\bs{Xdd}^\\text{T})^\\text{T})\n",
    "$$\n",
    "\n",
    "Since the trace is invariant under [cyclic permutations](https://en.wikipedia.org/wiki/Cyclic_permutation), we can write:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bs{d}^* &= \\argmin{d} \\Tr{((\\bs{X}-\\bs{Xdd}^\\text{T})^\\text{T}}(\\bs{X}-\\bs{Xdd}^\\text{T}))\\\\\\\\\n",
    "&=\\argmin{d} \\Tr{((\\bs{X}^\\text{T}-(\\bs{Xdd}^\\text{T})^\\text{T})}(\\bs{X}-\\bs{Xdd}^\\text{T}))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And $(\\bs{Xdd}^\\text{T})^\\text{T}=(\\bs{d}^\\text{T})^\\text{T}\\bs{d}^\\text{T}\\bs{X}^\\text{T}=\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}$. Let's plug that into our equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bs{d}^* &= \\argmin{d} \\Tr{(\\bs{X}^\\text{T}-\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T})}(\\bs{X}-\\bs{Xdd}^\\text{T}))\\\\\\\\\n",
    "&= \\argmin{d} \\Tr{(\\bs{X}^\\text{T}\\bs{X}-\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T} -\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{X} +\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T}})\\\\\\\\\n",
    "&= \\argmin{d} \\Tr{(\\bs{X}^\\text{T}\\bs{X})} - \\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "- \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{X})} + \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can remove the first term that does not depend on $d$:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmin{d} - \\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "- \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{X})} + \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "$$\n",
    "\n",
    "And because of the cycling property of a trace, we have:\n",
    "\n",
    "$$\n",
    "\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})} = \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{X})}\n",
    "$$\n",
    "\n",
    "We can simplify to:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmin{d} -2\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    " + \\Tr{(\\bs{d}\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    "$$\n",
    "\n",
    "and then\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmin{d} -2\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    " + \\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T}\\bs{d}\\bs{d}^\\text{T})}\n",
    "$$\n",
    "\n",
    "Because of the constraint $\\bs{dd}^\\text{T}=1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bs{d}^* &= \\argmin{d} -2\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\n",
    " + \\Tr{(\\bs{X}^\\text{T}\\bs{Xd}\\bs{d}^\\text{T})}\\textrm{ subject to }\\bs{dd}^\\text{T}=1\\\\\\\\\n",
    "&= \\argmin{d} -\\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\\textrm{ subject to }\\bs{dd}^\\text{T}=1\\\\\\\\\n",
    "&=\\argmax{d} \\Tr{(\\bs{X}^\\text{T}\\bs{Xdd}^\\text{T})}\\textrm{ subject to }\\bs{dd}^\\text{T}=1\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and with the cycling property:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmax{d} \\Tr{(\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xd})} \\textrm{ subject to }\\bs{dd}^\\text{T}=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "\n",
    "We will see that we can find the maximum of the function by calculating the eigenvectors of $\\bs{X^\\text{T}X}$.\n",
    "\n",
    "\n",
    "### Covariance matrix\n",
    "\n",
    "As we wrote above, the optimization problem of maximizing the variance of the components and minimizing the error between the reconstructed and the actual data are equivalent. Actually, if you look at the formula of $\\bs{d}$ you can see that there is the term $\\bs{X^\\text{T}X}$ in the middle.\n",
    "\n",
    "If we have centered our data around 0 (see bellow for more details about centering), $\\bs{X^\\text{T}X}$ is the covariance matrix (see [this Quora question](https://www.quora.com/Why-do-we-need-to-center-the-data-for-Principle-Components-Analysis)).\n",
    "\n",
    "The covariance matrix is a $n$ by $n$ matrix ($n$ being the number of dimensions). Its diagonal is the variance of the corresponding dimensions and the other cells are the covariance between the two corresponding dimensions (the amount of redundancy).\n",
    "\n",
    "<img src=\"images/covariance_matrix.png\" >\n",
    "\n",
    "The larger the covariance we have between two dimensions, the more redundancy exists between these dimensions. This also means that the best-fit line is associated with small errors if the covariance is high. To maximize the variance and minimize the covariance (in order to decorrelate the dimensions) means that the ideal covariance matrix is a diagonal matrix (non-zero values in the diagonal only). Therefore the diagonalization of the covariance matrix will give us the optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example 2.\n",
    "\n",
    "As an example we will create a 2D data set. To see the effect of the PCA we will introduce some correlations between the two dimensions. Let's create 100 data points with 2 dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "x = 5*np.random.rand(100)\n",
    "y = 2*x + 1 + np.random.randn(100)\n",
    "\n",
    "x = x.reshape(100, 1)\n",
    "y = y.reshape(100, 1)\n",
    "\n",
    "X = np.hstack([x, y])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X[:,0], X[:,1], '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly correlated data means that the dimensions are redundant. It is possible to predict one from the other without losing much information.\n",
    "\n",
    "The first processing we will do is to center the data around 0. PCA is a regression model without intercept (see [here](https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca)) and the first component must therefore cross the origin.\n",
    "\n",
    "Here is a simple function that substracts the mean of each column from each data point within the column. It can be used to center the data points around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerData(X):\n",
    "    X = X.copy()\n",
    "    X -= np.mean(X, axis = 0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's center our data $\\bs{X}$ around 0 for both dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAD3CAYAAADykopzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAF81JREFUeJzt3X9wnVWdx/F3aEp/Jm22uTTb2LRA9UTsD+0gqC0s7GoZXSzIjKtinRVhGFhxBEaWxVmG3dUZhnEVGZfuukJnd5SKokIdVrSzimxb1BZhanG9R1tKWom1SSZNU9KEJpv9I72Xm9v73HufH/f5+XnNMJOb3Ps8Jynn+5wf33NO0+TkJCKSTWdFXQARiY4CgEiGKQCIZJgCgEiGKQCIZFhzWDfq6xsOdbqhrW0ug4MjYd7SNZUxOEkoZ1RlzOVampx+ltoWQHPzjKiLUJPKGJwklDOOZUxtABCR2hQARDJMAUAkwxQARDJMAUAkwxQARGIi3zNIvmcw1HuGlgcgItVt23kQgO5lbaHdUy0AkYjlewa575HnsYePYQ8f475HnndsCQTdSlALQCRi3cvaaJk7k7sf3g3ApisMne3zKr630Eoo/awfCgAiMbAnf5SN65YD8Fz+KJ3rz53283zPINt2HsQePgbAV773K9oXzOEfP3GRr/uqCyCpFsXAmheduflcfcl5XH3JeSyp8PTvXtbGpg1vKr4+OTbB4aMnqnYX6qEWgKRaFANrXry9+5yKX5cqtBKOj7zGT1/oBap3F+qhFoCkkpuBtaQotBJa557NhSbHxnXLeS5/1Nc11QKQVHIzsJYUhZZBIRDAVKvADwUASa1aA2tJVU93oV4KAJJanbn5xQri90mZVhoDkNQK8kmZVgoAIhmmACCSYQoAIgnRiKQmDQKKJEQjkprUAhCJuUYmNSkAiMRc+TqATVeYwFoB6gKIBKzwdA6yqd6opCYFAJGANaKv3qikJnUBJJMaMaLeyL56o5KafLUAjDHnAL8E3mOtzQdTJJH6FSpYLtfi6nONeEoncQGS5wBgjJkJfBU4GVxxRNwpVORLLuyq6/3lO+vc98jzXLX+3MACQdIWIPlpAfwz8G/AXQGVRaRu5RX5rs07ed9FXTUrcqOf0klbgNQ0Oen+1G5jzMeBN1hrP2+M+SlwU60uwPj4xGQcT0eV5Oo5cpxbvvA0AA/ecTldHa11fW7rj17/X7UJ+MgV3Y0oXpw4Hg/uNQD8DzB5+r+3Ar8FNlprjzh9pq9v2P2NfMjlWujrGw7zlq6pjP48seOl4tfz583i3Ws76/rcnvzRaU/psFYKRvW3zOVaHAOApy6AtfbSwtclLQDHyi/SCKXNbdt7vO7PhbVMuBH5AEFTHoAkVmnlXb+mM3YtlfI9/N3OVITBdwCw1l4WQDlEUqPiHv6ts+ueqQiTEoFEAlZxD/++V7lr887Y7UysLoCIB7X694V8gOGR13j69B7+N1+zmjkzmuq+RhgUAEQ8qJVJWBigfGLHS1xocixpn8euvb3TZiricGiJAoCIC/VmElbaw78wU9HobEQ3NAYg4oLbtfnlMxVertFIagGIuBREvn9c1gwoAIi4FES+f1zWDKgLIOJSEJmEcTm0RAFAJMMUAEQyTAFAJMMUACQyjdiXr5FqlTdpvw9oFkAi5DUTLqoU2lrljUNmn1sKABI6v5lwYVe0WuWNU2afW+oCSOi8ZsI1ctvtamqVN06ZfW6pBSCR8JIJF+W227XKG5fMPrcUACQSXjPhoqpotcobl8w+txQAJBJeM+Giqmi1yhuXzD63NAYgiZLUihZXCgAiGaYAIKmTxIScqGgMQEJRnrzTyGSeJCbkREUBQEJRXikbUUmTnJATFQUAaajySnnPlt0wOcnhvleBYCtpEo/njprGAKShyrPkbtz4Fm7c+Jbi66Cz5gp5AhvXLee5BM3HR0UtAGm48uSdycnJhiXzJDUhJyqeAoAxZiawBVgOzAI+b639foDlkhSpVCm9VNJ6Bg7f3n1O8X1ZP/izHl5bAJuAAWvtx4wxi4AXAAUAqaha8o6bSlrvwGEYswBpmWnwGgAeA75T8no8gLKIVFTv6H6lAcd1KzvYcFFwh3KGcY8wNU1OTnr+sDGmhakn/9estVurvXd8fGKyuXmG53tJtvUcOc4tX3gagAfvuJyujtaa71uxdAGzz27m3r9Z37CyNOoeAWty/IHXAGCMWQo8Dmy21m6p9f6+vmHvkcaDXK4ldufFl1MZp6vWr35ix0vFr5uamriqbOCwUM4ndrxE/9AoL740wPGRUwCYpQsDzQco3MMeGmTg+Fjd94jq3zuXa3EMAF4HARcD24FbrLU/9lowEXi94lfrV9c7ul84i++f/mNPMQAEnQ9QuMcrfScSn3PgqQVgjHkA+BCQL/n2e621J50+oxbAmZJSxh3PHQL8D3g5PeHv2bKb/qGTnBybALw9sY8MjXHs2AjAtD76otZZdHe1cf2VF/gqeyW1WiXlUtMCsNZ+Gvi05xJJogQ14l1+ncKA2uGjJ6a9r9bTtHyhT/eyNrZuz3PqtQnu/OjaadmAt/7VW+ntf9XxGn5+pzTkHPgaBHRDLYAzxb2M+Z5BfrD7EC8eGAC896XLR85Lr1PajL78bUtomXt2zafpfY88D8DI6ClGX5vgT1pnT7v2ogWzaF8wB3B+MheucedH17r6XfxITQtAsqF7WRtdb1hYHPH22s+tlqO/J3+Ud61cTP/QKK3zZnHV+nMdn6blgaRg3pyZxa83XWHY85s/0t3VRveytjOupQVD02ktgFS1a29vILn1Tjn6nbn5DAyN0UQTS04HhUrJQYUme+m6goJjJ8b48wuXFq9tDx0rdjfKr5XkHXwbQS0AqaqrowWzpBPw18+t1F/O9wzyk1/+ftoTvWXOzIoVslChTddCNq5bzt4D/fQcOVG89m0fWcuO5w7xje2W3oGpwUCnp3tSd/BtBAUAqWr9ms5iv9VPbn2ldOB6lu+WN9mHR15j0wZDb/+r5BbMYUn7vGKF717Wxs1Xr6w5NZeGwbugKABIpApjAFD5aVweJG7+wCo62+cxfPJUxUpcz9NdG4u+TgFAAuNlaq0zNx97aOrpvmZFruJ7KlVqp0qsp7s7CgASGLf5AvWOAbip1Hq6u6NZAPHN65l99Y7Iq1I3jgKA+FarIlfbpltbeEVLXQAJRLXBt3oW+eR7Bskf0l7+YVMAkEA4zfPXyrorfKYQJCRc6gKkXFin5DjN89fq43sdPyj9vE4B8k4tgJSLeu+6WvPyfvfyL/x+l1yYzC25oqYAkFJxWfRSzxSel9Tc8t/vrs07ed9FXZnO6/dCASCl4nJKTj1TeF6Sd87IELxmNXNmOK56FQfaDyBCjS6j2x1r4Mxsvjj/HUt/v/nzZvHutZ0RlqY27QcgofLyZPWSzefm/UEq/f1s7/HQ758GCgAp5iaDrtKYwdve2M5qs5iOBbMqvh+iHWQs/Z1KVy1K/TQNKEDlKbsXftfP1u35iu//5n//lq9871eep+8kHtQCSDG3zfPCaHz/0Chf/vZeBo6PAtNnEIobefZN32gzqdtiZ50CQIq5bZ6X9ql/tPsQ3/rJfmB65S4ffb/8bZ20zJ2Z+Z11kkoBIIW85gCU9qlPjo2zcd1y5s2bdUblLrQUevtfpXXe2VU38pR4UwBIoSByAAqtgVyuhR/sOFDxZ3Dmcd8FTt2PtByrnRYKACnld+PLeo/0dppdcOp+RJ2aLNMpAKRUVFtjOXU/gFikJst0ngOAMeYsYDOwBhgDbrDW7g+qYOJP+VM6rKZ3te5HHFKTZTo/LYCrgdnW2ncaY94BfBG4KphiSdDCbHo7dT+0H3/8+AkA64EfAlhrf26MuTCYIkmQolgV6NT90I698eN5MZAx5iHgu9bap06/PgScZ60dr/T+8fGJyebmGZ4LKt71HDlePN/vwTsup6ujNeISScgashjoONBS8vosp8oPMDg44uNW7sV5FVtBWGXc/uzBYtN7+89ermtVYEES/o6QjHJGuBrQ8Wd+AsAu4P3At0+PAezzcS0JWOmgn5re4sRPAHgceI8x5lmmmhjXBVMkKXAzcl/+3tJBP+2rL048BwBr7f8BNwVYFilTWolrBYPSXXUbNeinLL70USJQDFUauR8ZPcXc2Wcem1X+3m07D7JuVUfxdZDz7criSx/tBxBD5Wvzx05NcLjv1Yrr7iut4+8fGg30tJ16tu7W9tzJpBZATJUmzQyPnOLlI1Ojx5We6Gck2AQ86FfP4iK1DpJJASCmSivx5sf3Vc2gK6/wpQN9LXNmku8ZdF0xC0/zwhSSUxZfXLYfF28UAGJq2sj9mxdXfaJXG+X3+mQuP3DDqVURl+3HxRttCx6hRpax/Mlsli6s68lc/rmV5y+qeeDGEzteon9oavuw3MI5rhKNgpL1f+8a93XMBNQgYErVcy5fPZ+7+ZrVNT/XmZvPwNAoA0OjLNHTP1HUBUgxr6vvSj+3a29v1QM38j2D/OSXvy+2GGBq3EFjAMmgAJBiXmcD3By4oTGAZFMXIMUKlTjfM0jLnJmuPwdTB27UUmgxBJV3IOFRCyAh/KThNnqOXouNkksBICG8VOKw5ui12Ci51AWIWK0U2nrScJ14nQmQ7FAAiNi2nQenreQrVajo1SpxrQCi/rlUoy5ARPI9g3zpsb3F5vk9W3azbmUHGy7qKr6nEBhM10LH6bxaXQM3/XMt980etQAi0r2sjZuuWV18PeOsJl74XT9wZrN/174/MHdWM1dfcl4x0aberoGb/nm11oikkwJAhHbt7eVdKztY1DqLl48MFysyTG/2t8w9uxgcCpU4yP69n3EGSTYFgAh1dbRww5UXcOsH1xS/V6jIe/JHedfKxWcEh9KKGVT/XoOF2aUxgAitX9NJX99wxZTdztx8rr7kPF7pO+GYZRfk/LsO7cgmBQAfgho0q3babrWKGeT8u9tgogHDdFAA8MFvht22Zw5w4sTotJH/8opcb8X0WyHdBhPtAJQOCgAeBJVht3V7nsnJyWkBoFy9FTOsCqkdgNJFg4Ae+B002777EJ+8/xlGRsc5OTbBJ+9/hq//KO9p5D3sEXwNGKaLAoBHfkbgN1zUxfV/+ebi6xuuvIDe/hFPc/BRVEhlF6aHugAe+R2B377nMCvPW8Sx4VEeevI3nBybOlbRS5M67BF8rf5LDwUAj/yOwC/vaOVTH15LX98wDz35vzz74hHA24YaYVdIrf5LD08BwBizAPgG0AqcDdxurf1ZkAULQ9hTWaX3+/BfvLH4/fYFs309wcsrpKbopF5eWwC3Az+21n7ZGGOAbwJrgytWOMKeynK6X9BPcE3RSb28BoD7gbGSa4wGU5xwhD2V5XS/wqEbQTWpNUUnbtU8F8AYcz1wW9m3r7PW7jHGdABPAbdaa5+pdp3x8YnJ5uYZvgpby779UwtmVq1or/neniPHueULTwPw4B2X09XR2tD7N+J+lYR1H0kUx3MBPB8MYoxZBTwKfMZa+1St94dxMEhhJd2dH11b8xCGJ3a8VPy6qakpkMMsSu9fz/0acVBE0L9XEg7cgGSUM44Hg3gdBLwAeAz4kLV2r9eCBaVS0/evr3wLHQtmOQ6IBdnvrqfpHdZIvaboxA2viUD3ArOBB4wxPzXGbAuwTK5VSoYpNMOdNrkIciqrnmScsKbONEUnbqTmbMDypu/Fq5bwn0/+2vXZeF499OSvAWhfMKfuprearcFJQjnj2AVITSpwYf18YdusVSvaG54iW7oh58t/GGZgaGzatl1hqbUxqIiT1GQCVmr6BpEiWy2pZtvOg4yMnmLu7Jn0DozAwEix/x8mzfuLV6kJAJUEMSBWqXKVD/ot72gp/izIs/FqZfRp3l/8SnUA8DMgVq1ylR+Iee6ftrL6/EVAsItxaj3ZdTCn+JWqABBkDnytylXavegdGOFjV5ji9/1y82Svp5ujtQHiJFUBIOi+cLXK5dS9CGLqzc2TvZ5ujsYIxEkqpgHLn5hm6cJiIpAfe/JHp1WuoOfVC9NClZ7QQWT0Vfq7uB0jSML0GiSjnHGcBkxFC6DSE3PVinbff+ywkmoqPaGDGMDUGIHUkooWAJz5xLzhA6tj/0Q4MjTW8GQlvy2JJDxZIRnlVAuggZKYA19IVmrkEzqJfxcJT2oCQFJz4Bu9n19S/y4SjkQEgDRPY+kJLVFKRACI+zSWnwClJ7REKdYBIIpU13zPIIf+OEzX4pa67xP3ACXiJNYBIIpprG07D3Lo6DBd59QOAMrFl6SL/XLgsE6hyfcMcs+W3djDxzg5NoE9fIx7Hv5F1WW2OiZLki72AaB8nX+jdC9r48b3XzDtezdetbJmhdYxWZJkse4CQLiDZHvyR3nT0gUANNFU17Sc21H8NM9oSPLEPgCEqTM3v1ih652ScxugNGAocaIAUKKRrY1qOxcHdX1QYBF3Yj8GkBbVdi4OgtPuxyLVqAUQovK037e+ucP3NTUVKX4oAISoEWm/WvIrfigAhKhRYwyNXlAk6aUAkAJaUCReaRAwBbSgSLzy1QIwxnQDvwAWW2tHgymSiITFcwvAGNMKfBEYC6444dKRWpJ1Xo8HbwL+HfgsEOnJwH4oK0+yruamoMaY64Hbyr7dAzxqrf26MeZloLtWF2B8fGKyuXmGj6IGZ9/+frZuz/PigQEAVp6/iGs3dAeamCMSI46bgnraFdgYsx/4/emX7wB2W2svrfaZRu8KXK7WDqyv9J0ozp1/7oaLI5k71062wUlCOVOzK7C1dkXh69MtgA1erhMlzZ2LZHgasLDPQHdXG2GdjSASN74Tgay1ywMoR0NUWyFXmC/XAhrJslRnAlYb5dciGpGUdgHyPYPctXkn9vAx7OFj3PfI82fM92s/P5GUBoDuZW3cdM3q4munyq39/CTrUtsF2LW3t+YovxbRSNbFKgAEua1VV0cLZkkn4Fy5tYhGsi5WASDI1Nz1azqLSReq3CKVxWIMIN8zyH2PPF910E5EgheLAKAReZFoxKYLoNRckfDFJgBoRF4kfLHoAoBG5EWiEJsAICLhUwAQyTAFAJEMUwAQyTAFAJEMUwAQyTAFAJEMUwAQyTAFAJEMUwAQyTAFAJEMUwAQyTAFAJEMUwAQyTAFAJEM87QhiDFmBvAl4EJgFvAP1tongyyYiDSe1xbAx4CZ1tp1wFXAihrvb6h8z6A2ERXxwOuWYFcA+4wx/wU0AZ8KrkjuBbmduEiWNNU6GtsYcz1wW9m3+4CXgU8AlwKfs9ZeWu064+MTk83NM7yXtIJ9+/vZuj3PiwcGAFh5/iKu3dDNqhXtgd5HJOGaHH9QKwBUYox5FHjMWvvd06+PWGs7qn2mr2/Y/Y3q8ErfCe5+eDcAn7vhYjrb5wGQy7UUDwaJK5UxOEkoZ1RlzOVaHAOA1zGAncD7AIwxa4BDHq/jmw74FPHO6xjA14B/Ncb8nKnmxU3BFckdbScu4p2nAGCtHWOq/x+68gNEtZ24iHexORikXhrxFwlOYjIBdYCoSPASEwB0gKhI8BLVBdABoiLBSlQA0Ii/SLAS0wUAjfiLBC1RAUBEgqUAIJJhCgAiGaYAIJJhCgAiGaYAIJJhnvYDEJF0UAtAJMMUAEQyTAFAJMMUAEQyTAFAJMMUAEQyTAFAJMMStR9AvYwxC4BvAK3A2cDt1tqfRVsqZ8aYDwAftNZeG3VZCowxZwGbgTXAGHCDtXZ/tKWqzBhzMXCftfayqMtSiTFmJrAFWM7UWZqft9Z+P9JCnZbWFsDtwI+ttX8GfBx4MNriODPGPADcS/z+La4GZltr3wn8HfDFiMtTkTHmb4GHgNlRl6WKTcCAtfYS4L3Av0RcnqK4/U8XlPuBr57+uhkYjbAstTwL3Bx1ISpYD/wQwFr7c6ZOgo6jA8A1UReihseAu0tej0dVkHKJ7wI4nF14nbV2jzGmg6muwK3hl2y6KuX8ljHmsgiKVEsrMFTyesIY02ytjc3/vADW2u8aY5ZHXY5qrLUnAIwxLcB3gL+PtkSvS3wAsNY+DDxc/n1jzCrgUeAz1tpnQi9YGadyxthxoKXk9Vlxq/xJYoxZCjwObLbWbo26PAWp7AIYYy5gqtl1rbX2qajLk1C7eP38x3cA+6ItTnIZYxYD24E7rbVboi5PqcS3ABzcy9Sg0APGGIAha+1V0RYpcR4H3mOMeZap8x+vi7g8SfZZoA242xhTGAt4r7X2ZIRlArQcWCTTUtkFEJH6KACIZJgCgEiGKQCIZJgCgEiGKQCIZJgCgEiG/T9Hq3eb6377aQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_centered = centerData(X)\n",
    "plt.plot(X_centered[:,0], X_centered[:,1], '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the middle of our y-axis is now $0$.\n",
    "\n",
    "We can now look for PCs. We saw that they correspond to values taken by $\\bs{d}$ that maximize the following function:\n",
    "\n",
    "$$\n",
    "\\bs{d}^* = \\argmax{d} \\Tr{(\\bs{d}^\\text{T}\\bs{X}^\\text{T}\\bs{Xd})} \\textrm{ subject to }\\bs{dd}^\\text{T}=1\n",
    "$$\n",
    "\n",
    "To find $\\bs{d}$ we can calculate the eigenvectors of $\\bs{X^\\text{T}X}$. So let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.91116273, -0.41204669],\n",
       "       [ 0.41204669, -0.91116273]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVals, eigVecs = np.linalg.eig(X_centered.T.dot(X_centered)/100)\n",
    "eigVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 18.04730409, 798.35242844]), array([[-0.91116273, -0.41204669],\n",
      "       [ 0.41204669, -0.91116273]]))\n",
      "(array([0.18047304, 7.98352428]), array([[-0.91116273, -0.41204669],\n",
      "       [ 0.41204669, -0.91116273]]))\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.eig(X_centered.T.dot(X_centered)))\n",
    "print(np.linalg.eig(X_centered.T.dot(X_centered)/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the vectors maximizing our function. Each column vector is associated with an eigenvalue. The vector associated with the larger eigenvalue tells us the direction associated with the larger variance in our data. To check that, we will plot these vectors along with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orange = '#FF9A13'\n",
    "blue = '#1190FF'\n",
    "plotVectors(eigVecs.T, [orange, blue])\n",
    "plt.plot(X_centered[:,0], X_centered[:,1], '*')\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the blue vector direction corresponds to the oblique shape of our data. The idea is that if you project the data points on the line corresponding to the blue vector, you'll maximize the explained variance. Have a look at the following figure:\n",
    "\n",
    "<img src=\"images/principal-component-analysis-variance-explained.png\" width=\"400\" alt=\"Representation of the variance explained across directions\" title=\"Maximizing the variance\">\n",
    "<em>Projection of the data point: this line direction is the one with the largest variance</em>\n",
    "\n",
    "When you project data points on the pink line there is more variance. This line has the direction that maximizes the variance of the data points. It is the same for the figure above: our blue vector has the direction of the line where data point projection has the higher variance. Then the second eigenvector is orthogonal to the first.\n",
    "\n",
    "In our figure above, the blue vector is the second eigenvector so let's check that it is the one associated with the bigger eigenvalue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18047304, 7.98352428])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yes, the second vector corresponds to the biggest eigenvalue.\n",
    "\n",
    "Now that we have found the matrix $\\bs{d}$ we will use the encoding function to rotate the data. The goal of the rotation is to end up with a new coordinate system where data is uncorrelated and thus where the basis axes gather all the variance. It is then possible to keep only few a axes: this is the purpose of dimensionality reduction.\n",
    "\n",
    "Recall that the encoding function is:\n",
    "\n",
    "$$\n",
    "\\bs{c}=\\bs{D}^\\text{T}\\bs{x}\n",
    "$$\n",
    "\n",
    "$\\bs{D}$ is the matrix containing the eigenvectors that we have calculated before. In addition, this formula corresponds to only one data point where dimensions are the rows of $\\bs{x}$. In our case, we will apply it to all data points and since $\\bs{X}$ has dimensions on the columns we need to transpose it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_new = eigVecs.T.dot(X_centered.T)\n",
    "\n",
    "plt.plot(eigVecs.T.dot(X_centered.T)[0, :], eigVecs.T.dot(X_centered.T)[1, :], '*')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! The rotation transformed our dataset so that we now have the more variance on one of the basis axes. You could keep only this dimension and have a fairly good representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the unit norm constraint\n",
    "\n",
    "We saw that the maximization is subject to $\\bs{dd}^\\text{T}=1$. This means that the solution vector has to be a unit vector. Without this constraint, you could scale $\\bs{d}$ up to infinity to increase the function to maximize (see [here](https://stats.stackexchange.com/questions/117695/why-is-the-eigenvector-in-pca-taken-to-be-unit-norm)). For instance, let's see some vectors $\\bs{x}$ that could maximize the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4165298.04389264]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([[12], [26]])\n",
    "d.T.dot(X.T).dot(X).dot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this $\\bs{d}$ does not have a unit norm (since $\\bs{d}$ is a column vector we use the transpose of $\\bs{dd}^\\text{T}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[820]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.T.dot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors have unit norm and thus respect the constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVecs[:,0].dot(eigVecs[:,0].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigVecs[:,1].dot(eigVecs[:,1].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "## PCA\n",
    "\n",
    "- Visualize PCA:  http://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "- A lot of intuitive explanations on PCA: https://arxiv.org/pdf/1404.1100.pdf\n",
    "\n",
    "- https://brilliant.org/wiki/principal-component-analysis/#from-approximate-equality-to-minimizing-function\n",
    "\n",
    "- http://www4.ncsu.edu/~slrace/LinearAlgebra2017/Slides/PCAPrint.pdf\n",
    "\n",
    "- https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
    "\n",
    "- https://www.cs.bgu.ac.il/~inabd171/wiki.files/lecture14_handouts.pdf\n",
    "\n",
    "## Semi-orthogonal matrix\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Semi-orthogonal_matrix\n",
    "\n",
    "## Intuition about PCA\n",
    "\n",
    "- https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/\n",
    "\n",
    "## Derivatives\n",
    "\n",
    "- https://math.stackexchange.com/questions/1377764/derivative-of-vector-and-vector-transpose-product\n",
    "\n",
    "## Link between variance maximized and error minimized:\n",
    "\n",
    "- https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation\n",
    "\n",
    "- https://stats.stackexchange.com/questions/32174/pca-objective-function-what-is-the-connection-between-maximizing-variance-and-m\n",
    "\n",
    "- https://stats.stackexchange.com/questions/318625/why-do-the-leading-eigenvectors-of-a-maximize-texttrdtad\n",
    "\n",
    "## Centering data\n",
    "\n",
    "- https://www.quora.com/Why-do-we-need-to-center-the-data-for-Principle-Components-Analysis\n",
    "- https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca\n",
    "\n",
    "## Unit norm constraint\n",
    "\n",
    "- https://stats.stackexchange.com/questions/117695/why-is-the-eigenvector-in-pca-taken-to-be-unit-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
